# -*- coding: utf-8 -*-
"""PrecisionFar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vmyI3ScZMWkaI7gwmwryaOeXnC4_gT9m
"""

import joblib
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load datasets
first_dataset = pd.read_csv('/content/SoilData1.csv')
second_dataset = pd.read_csv('/content/indiandataset1.csv')

# Prepare features and target variable
features = ['Temparature', 'Moisture', 'Humidity']
X = second_dataset[features]
y = second_dataset['Crop Type']

# Add polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the models
gbc = GradientBoostingClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)

# Define hyperparameters grid for tuning
param_grid_gbc = {
    'n_estimators': [50, 100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
    'max_depth': [3, 5, 7, 9]
}

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Perform Grid Search with Cross-Validation
grid_search_gbc = GridSearchCV(gbc, param_grid_gbc, cv=5, scoring='accuracy')
grid_search_gbc.fit(X_train, y_train)
best_gbc = grid_search_gbc.best_estimator_

grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train, y_train)
best_rf = grid_search_rf.best_estimator_

# Create an ensemble model
ensemble = VotingClassifier(estimators=[('gbc', best_gbc), ('rf', best_rf)], voting='soft')
ensemble.fit(X_train, y_train)

# Save the ensemble model and scaler
joblib.dump(ensemble, 'ensemble_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(poly, 'poly.pkl')

# Evaluate the ensemble model
y_pred = ensemble.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Ensemble Model Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Predict crop types for the first dataset
first_dataset_poly = poly.transform(first_dataset[features])
first_dataset_scaled = scaler.transform(first_dataset_poly)
crop_predictions = ensemble.predict(first_dataset_scaled)
print("Predicted crop types for the first dataset:")
print(crop_predictions)